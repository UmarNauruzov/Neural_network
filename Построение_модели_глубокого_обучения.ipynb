{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UmarNauruzov/Neural_network/blob/main/%D0%9F%D0%BE%D1%81%D1%82%D1%80%D0%BE%D0%B5%D0%BD%D0%B8%D0%B5_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8_%D0%B3%D0%BB%D1%83%D0%B1%D0%BE%D0%BA%D0%BE%D0%B3%D0%BE_%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D1%8F.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDQzNIZXAoFE"
      },
      "source": [
        "Зададим гиперпараметры модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOMw2ZbOAmOZ"
      },
      "source": [
        "epsilon = 1 # Параметр эпсилон при использовании эпсилон жадной стратегии\n",
        "gamma = 0.9 # Коэффциент дисконтирования гамма\n",
        "random_seed = 1212131 #Random seed\n",
        "time_delay = 1 # Задержка времени при отрисовке процесса игры после обучения (секунды)\n",
        "lr_rate = 0.9 #Коэффициент скорости обучения альфа"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQu5IYHX8jId"
      },
      "source": [
        "Импортируем библиотеки, создаем свою среду размера 6х6. S обозначает точку старта. F -- лед безопасен, H -- проталина, G -- цель. Параметр `is_slippery=False` отвечает за условное отсутствие скольжения. То есть если агент выбрал действие пойти направо, то он переместится в соответствующее состояние. В общем случае из-за \"скольжения\" можно оказаться в другом состоянии. Мы также скопировали из библиотки GYM и слегка модифицировали функцию ```generate_random_map ```, для того, чтобы генерировать произвольные карты на основе ```random_seed ```.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awL7CCCwD6C3",
        "outputId": "601f7898-6d05-4cc5-de90-78562727cbef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -q --upgrade gym==0.18.0\n",
        "import gym\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "\n",
        "\n",
        "def generate_random_map(size, p, sd):\n",
        "    \"\"\"Генерирует случайную действительную карту (ту, которая имеет путь от начала до цели)\n",
        "      :размер параметра: размер каждой стороны сетки\n",
        "      :параметр p: вероятность того, что плитка заморожена\n",
        "    \"\"\"\n",
        "    valid = False\n",
        "    np.random.seed(sd)\n",
        "\n",
        "    # DFS to check that it's a valid path.\n",
        "    def is_valid(res):\n",
        "        frontier, discovered = [], set()\n",
        "        frontier.append((0,0))\n",
        "        while frontier:\n",
        "            r, c = frontier.pop()\n",
        "            if not (r,c) in discovered:\n",
        "                discovered.add((r,c))\n",
        "                directions = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n",
        "                for x, y in directions:\n",
        "                    r_new = r + x\n",
        "                    c_new = c + y\n",
        "                    if r_new < 0 or r_new >= size or c_new < 0 or c_new >= size:\n",
        "                        continue\n",
        "                    if res[r_new][c_new] == 'G':\n",
        "                        return True\n",
        "                    if (res[r_new][c_new] not in '#H'):\n",
        "                        frontier.append((r_new, c_new))\n",
        "        return False\n",
        "\n",
        "    while not valid:\n",
        "        p = min(1, p)\n",
        "        res = np.random.choice(['F', 'H'], (size, size), p=[p, 1-p])\n",
        "        res[0][0] = 'S'\n",
        "        res[-1][-1] = 'G'\n",
        "        valid = is_valid(res)\n",
        "    return [\"\".join(x) for x in res]\n",
        "\n",
        "#Генерация карты\n",
        "random_map = generate_random_map(size=6, p=0.8, sd = random_seed) #Создаем свою карту\n",
        "env = gym.make(\"FrozenLake-v0\", desc=random_map, is_slippery=False) #Инициализируем среду\n",
        "print(\"Карта\")\n",
        "env.render() #Выводим карту на экран"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.1/39.1 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for Pillow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Карта\n",
            "\n",
            "\u001b[41mS\u001b[0mFFFFF\n",
            "FFFFFF\n",
            "FFFHFH\n",
            "FFFFHF\n",
            "FFFFFF\n",
            "HFHFFG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDCexoEU9a_c"
      },
      "source": [
        "Функции выбора действия и обновления таблицы ценности действий. Строчка *** используется для того, чтобы проверять ответы в openedx. Вне рамках академической задачи лучше использовать оригинальный метод класса `environment`, то есть:\n",
        "\n",
        "`action = env.action_space.sample()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5TbDqn6G_Pt"
      },
      "source": [
        "\n",
        "Функция ```learn()``` в результате ее вызова обновляет значение ценности текущего действия согласно алгоритму Q-обучения\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdQBpxaTOK7u"
      },
      "source": [
        "def choose_action(state):\n",
        "    action=0\n",
        "    if np.random.uniform(0, 1) < epsilon:\n",
        "        action = np.random.randint(0,env.action_space.n) #***\n",
        "    else:\n",
        "        action = np.random.choice(np.array(np.argwhere(Q[state, :] == np.amax(Q[state, :])).flatten().tolist()))\n",
        "    return action\n",
        "\n",
        "def learn(state, state2, reward, action, done):\n",
        "    if done is True:\n",
        "        Q[state, action] = Q[state, action] + lr_rate*(reward - Q[state, action])\n",
        "    else:\n",
        "        Q[state, action] = Q[state, action] + lr_rate * (reward + gamma*np.max(Q[state2,:])-Q[state, action])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0adDl7NvJoQP"
      },
      "source": [
        "Функция ```env.step(action)```\n",
        "\n",
        "```state2``` -- следующее состояние\n",
        "\n",
        "```reward``` -- награда\n",
        "\n",
        "```done``` -- флаг окончания игры. True в случае победы или падения в проталину. False в остальных случаях.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aq92-dWiOchF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41aea14f-66fb-4c37-9625-c6fded349704"
      },
      "source": [
        "from tqdm import tqdm\n",
        "# Inititalization\n",
        "np.random.seed(random_seed)\n",
        "total_games = 10000\n",
        "max_steps = 100\n",
        "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "#Main cycle\n",
        "game1= [0]*total_games\n",
        "for game in tqdm(range(total_games)):\n",
        "    state = env.reset()\n",
        "    t = 0\n",
        "    while t < max_steps:\n",
        "        \n",
        "        t += 1\n",
        "\n",
        "        action = choose_action(state)\n",
        "\n",
        "        state2, reward, done, info = env.step(action)\n",
        "\n",
        "        if t == max_steps:\n",
        "          done = True  \n",
        "\n",
        "        learn(state, state2, reward, action, done)\n",
        "\n",
        "        state = state2\n",
        "\n",
        "        if done:\n",
        "            if reward == 1:\n",
        "                game1[game]=1\n",
        "            break\n",
        "\n",
        "def get_fg_and_v(game):\n",
        "    #количество побед\n",
        "    pobeda = 0\n",
        "    #количество побед подряд\n",
        "    pp=0\n",
        "    i=0\n",
        "    l=[]\n",
        "    for g in game:\n",
        "        if g == 1:\n",
        "            pobeda=pobeda+1\n",
        "            if len(l) ==0:\n",
        "                l.append(i)\n",
        "            else:\n",
        "                if len(l) <5:\n",
        "                    if l[-1] - i == -1:\n",
        "                        l.append(i)\n",
        "                    else:\n",
        "                        l=[i]\n",
        "                    if len(l) == 5 and pp==0:\n",
        "                        pp = i+1\n",
        " \n",
        "        i=i+1\n",
        "    return pp, pobeda\n",
        "\n",
        "pp, pobeda = get_fg_and_v(game1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:12<00:00, 784.74it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "# Inititalization\n",
        "wins_arr = [] #delete\n",
        "np.random.seed(random_seed)\n",
        "total_episodes = 10000\n",
        "max_steps = 100\n",
        "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "min_episode = 0 #delete\n",
        "#Main cycle\n",
        "for episode in tqdm(range(total_episodes)):\n",
        "    state = env.reset()\n",
        "    t = 0\n",
        "    while t < max_steps:\n",
        "      #delete\n",
        "        if episode > 5 and wins_arr[episode-5] == 1 and wins_arr[episode-4] == 1 and wins_arr[episode-3] == 1 and wins_arr[episode-2] == 1 and wins_arr[episode-1] == 1 and min_episode ==0:\n",
        "          min_episode = episode\n",
        "        \n",
        "        t += 1\n",
        "\n",
        "        action = choose_action(state)\n",
        "\n",
        "        state2, reward, done, info = env.step(action)\n",
        "\n",
        "        if t == max_steps:\n",
        "          done = True  \n",
        "\n",
        "        learn(state, state2, reward, action, done)\n",
        "\n",
        "        state = state2\n",
        "\n",
        "\n",
        "        if done and reward == 1:\n",
        "          wins_arr.append(1) #record if won\n",
        "          break\n",
        "        if done:\n",
        "          wins_arr.append(0) #record if lost\n",
        "          break"
      ],
      "metadata": {
        "id": "eIBfnl8TwtnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(\"Таблица ценностей действий\")\n",
        "#print(np.round(Q,4))\n",
        "#Number of wins\n",
        "print(\"Количество побед в серии из 10 000 игр: \", np.sum(wins_arr))\n",
        "#Number of the episode\n",
        "print(\"Пять побед подряд впервые было одержано в игре \",min_episode)\n",
        "#print(\"Q-table\")\n",
        "#print(np.round(Q,2))\n",
        "#print(\"Number of wins: \", #your code here)\n",
        "#print(\"Number of the episode\", #your code here)"
      ],
      "metadata": {
        "id": "4G-DR7lhwx-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFuxsqdRLOS9"
      },
      "source": [
        "Вывод ответов при заданных параметрах"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZbJtFnhLa7w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e534a1c-b825-46ff-cb64-30ad582bce57"
      },
      "source": [
        "print(\"Количество побед в серии из 10 000 игр: \", pobeda)\n",
        "print(\"Пять побед подряд впервые было одержано в игре \", pp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Количество побед в серии из 10 000 игр:  129\n",
            "Пять побед подряд впервые было одержано в игре  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nazZaAbwQGBt"
      },
      "source": [
        "Произведем одну игру, чтобы проследить за действиями агента. При этом будем считать модель полностью обученной, то есть действия выбираются жадно, значения ценностей действий в таблице не обновляются."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ysllZjEQXLa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16eb8f7e-8ed7-4e09-c41d-4cb634d2374f"
      },
      "source": [
        "import time\n",
        "#Жадный выбор действий\n",
        "def choose_action_one_game(state):\n",
        "    action = np.random.choice(np.array(np.argwhere(Q[state, :] == np.amax(Q[state, :])).flatten().tolist()))\n",
        "    return action\n",
        "\n",
        "states=[]#Массив для сохранения состояний агента в течение игры\n",
        "t = 0\n",
        "state = env.reset()\n",
        "wn = 0\n",
        "while(t<100):\n",
        "  env.render()\n",
        "  time.sleep(time_delay)\n",
        "  clear_output(wait=True)\n",
        "  action = choose_action_one_game(state)  \n",
        "  state2, reward, done, info = env.step(action)  \n",
        "  states.append(state)\n",
        "  state = state2\n",
        "  t += 1\n",
        "  if done and reward == 1:\n",
        "    wn=1\n",
        "  if done:\n",
        "    break\n",
        "if wn == 1:\n",
        "  print(\"!!!Победа!!!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!!Победа!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x696NulpReFI"
      },
      "source": [
        "Отобразим маршрут"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKMCMdpOTcXy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "outputId": "2d063def-35e1-4426-e189-b1f0151db79b"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def make_maze_pic(maze):\n",
        "  maze_pic=[]\n",
        "  for i in range(len(maze)):\n",
        "    row = []\n",
        "    for j in range(len(maze[i])):\n",
        "      if maze[i][j] == 'S':\n",
        "        row.append(0)\n",
        "      if maze[i][j] == 'F':\n",
        "        row.append(0)\n",
        "      if maze[i][j] == 'H':\n",
        "        row.append(1)\n",
        "      if maze[i][j] == 'G':\n",
        "        row.append(0)\n",
        "    maze_pic.append(row)\n",
        "  maze_pic = np.array(maze_pic)\n",
        "  return maze_pic\n",
        "  \n",
        "\n",
        "#Make maze fit to plot\n",
        "maze_pic = make_maze_pic(random_map)\n",
        "nrows, ncols = maze_pic.shape\n",
        "\n",
        "#Arrays of picture elements\n",
        "rw = np.remainder(states,nrows)\n",
        "cl = np.floor_divide(states,nrows)\n",
        "if wn == 1:\n",
        "  rw = np.append(rw, [nrows-1])\n",
        "  cl = np.append(cl,[ncols-1])\n",
        "\n",
        "#Picture plotting\n",
        "fig, ax1 = plt.subplots(1, 1, tight_layout=True)\n",
        "ax1.clear()\n",
        "ax1.set_xticks(np.arange(0.5, nrows, step=1))\n",
        "ax1.set_xticklabels([])\n",
        "ax1.set_yticks(np.arange(0.5, ncols, step=1))\n",
        "ax1.set_yticklabels([])\n",
        "ax1.grid(True)\n",
        "ax1.plot([0],[0], \"gs\", markersize=40)  # start is a big green square\n",
        "ax1.text(0, 0.2,\"Start\", ha=\"center\", va=\"center\", color=\"white\", fontsize=12) #Start text\n",
        "ax1.plot([nrows-1],[ncols-1], \"rs\", markersize=40)  # exit is a big red square\n",
        "ax1.text(nrows-1, ncols-1+0.2,\"Finish\", ha=\"center\", va=\"center\", color=\"white\", fontsize=12) #Exit text\n",
        "ax1.plot(rw,cl, ls = '-', color = 'blue') #Blue lines path\n",
        "ax1.plot(rw,cl, \"bo\")  # Blue dots visited cells\n",
        "ax1.imshow(maze_pic, cmap=\"binary\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f391c1c3550>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}